{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOU+QuU2fy/hh7mIWmdz8kP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit999zzz/-Domain-Specific-LLM-for-Financial-Analysis/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "financial statement generation and summarization system:\n",
        "\n",
        "1. Data Ingestion and Preprocessing:\n",
        "\n",
        "Data Sources: The system ingests data from two primary sources:\n",
        "Transaction Data: Raw transactional data containing information about individual transactions, such as date, amount, account type, etc. (e.g., transaction_data.csv).\n",
        "Financial Reports: Textual financial reports containing detailed financial information and insights (e.g., financial_reports.csv).\n",
        "Preprocessing:\n",
        "Transaction Data: The process_transaction_data function cleans, transforms, and aggregates the raw transaction data into a structured format suitable for model input. This includes handling date formats, aggregating transactions by account type and month, and calculating summary statistics.\n",
        "Financial Reports: The textual financial reports are combined into a single corpus, split into training and validation sets, and tokenized using the GPT-2 tokenizer.\n",
        "2. Model Fine-tuning:\n",
        "\n",
        "Model Selection: The system leverages the pre-trained GPT-2 language model as the foundation for financial statement generation and summarization. GPT-2 is a powerful language model capable of learning complex patterns in text data.\n",
        "Fine-tuning: The pre-trained GPT-2 model is fine-tuned on a combination of the structured transaction data and the textual financial reports. This fine-tuning process adapts the model to the specific language and concepts relevant to financial statements and summaries.\n",
        "Custom Trainer: A custom CustomTrainer class is implemented to handle the training process and define a custom loss function tailored for the language modeling task.\n",
        "Dataset Loading: The datasets library is used to load a relevant financial summarization dataset from the Hugging Face Hub, providing additional training data and improving the model's understanding of financial language.\n",
        "3. Inference and Task-Specific Functions:\n",
        "\n",
        "Financial Statement Generation: The generate_financial_statement function takes structured transaction data as input and uses the fine-tuned GPT-2 model to generate a financial statement based on the provided data.\n",
        "Financial Report Summarization: The summarize_financial_report function takes a financial report text as input and utilizes the fine-tuned model to generate a concise summary of key financial insights.\n",
        "Query Answering: The answer_query function enables users to ask questions about financial topics, and the model attempts to provide answers based on its learned knowledge.\n",
        "4. Model Deployment and Usage:\n",
        "\n",
        "Model Saving: The fine-tuned GPT-2 model and tokenizer are saved to disk for future use.\n",
        "Example Usage: The code provides example scenarios demonstrating how the model can be used for financial statement generation, report summarization, and query answering.\n",
        "Overall Architecture Diagram:\n",
        "Key Components:\n",
        "Data Sources: Transaction data and financial reports.\n",
        "Preprocessing: Data cleaning, transformation, and aggregation.\n",
        "Model: Pre-trained GPT-2 language model.\n",
        "Fine-tuning: Adapting the model to financial data.\n",
        "Custom Trainer: Managing the training process.\n",
        "Inference Functions: Generating statements, summaries, and answering queries.\n",
        "Model Deployment: Saving and loading the fine-tuned model.\n",
        "This architecture enables the system to ingest financial data, learn patterns from it, and perform various financial tasks, such as generating financial statements, summarizing reports, and answering questions. By leveraging the power of GPT-2 and fine-tuning it on relevant data, the system can provide valuable insights and assist with financial analysis"
      ],
      "metadata": {
        "id": "qHF62sqQ1d6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC0DJmhCnQQc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Step 1: Data Preparation\n",
        "# Assume `transaction_data.csv` contains raw transactional data\n",
        "# and `financial_reports.csv` contains financial reports for analysis.\n",
        "\n",
        "transaction_data = pd.read_csv(\"transaction_data.csv\")\n",
        "financial_reports = pd.read_csv(\"financial_reports.csv\")\n",
        "\n",
        "# Process raw transactional data into a structured format for model input.\n",
        "def process_transaction_data(data):\n",
        "    # Ensure 'Transaction_Date' column exists and is valid\n",
        "    if 'Transaction_Date' not in data.columns:\n",
        "        raise KeyError(\"'Transaction_Date' column is missing in the input data.\")\n",
        "\n",
        "    data['Transaction_Date'] = pd.to_datetime(data['Transaction_Date'], errors='coerce')\n",
        "\n",
        "    # Drop rows with invalid or missing dates\n",
        "    data = data.dropna(subset=['Transaction_Date'])\n",
        "\n",
        "    # Create 'Month' column based on 'Transaction_Date'\n",
        "    data['Month'] = data['Transaction_Date'].dt.to_period('M')\n",
        "\n",
        "    # Aggregate data by 'Account_Type' and 'Month'\n",
        "    aggregated_data = data.groupby(['Account_Type', 'Month']).agg({\n",
        "        'Amount': ['sum', 'mean'],\n",
        "        'Transaction_ID': 'count'\n",
        "    }).reset_index()\n",
        "\n",
        "    aggregated_data.columns = ['_'.join(col).strip() for col in aggregated_data.columns.values]\n",
        "    return aggregated_data\n",
        "\n",
        "structured_transactions = process_transaction_data(transaction_data)\n",
        "structured_transactions.to_csv(\"structured_transactions.csv\", index=False)\n",
        "\n",
        "# Prepare labeled data for fine-tuning the model\n",
        "# Combine financial text data into a single corpus\n",
        "all_text = \"\\n\".join(financial_reports['Content'].dropna().tolist())\n",
        "\n",
        "# Split data into training and evaluation\n",
        "train_texts, val_texts = train_test_split(all_text.split(\"\\n\"), test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Tokenization\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Create Dataset objects for PyTorch\n",
        "class FinancialDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Add labels for language modeling task\n",
        "        item['labels'] = item['input_ids'].clone()  # Labels are the same as input_ids for language modeling\n",
        "        return item\n",
        "\n",
        "train_dataset = FinancialDataset(train_encodings)\n",
        "val_dataset = FinancialDataset(val_encodings)\n",
        "\n",
        "# Step 3: Model Fine-Tuning\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Resize token embeddings after adding a new pad token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",           # output directory\n",
        "    run_name=\"fine_tune_financial_model\",  # Custom run name\n",
        "    num_train_epochs=3,               # total number of training epochs\n",
        "    per_device_train_batch_size=4,    # batch size for training\n",
        "    per_device_eval_batch_size=4,     # batch size for evaluation\n",
        "    warmup_steps=500,                 # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                # strength of weight decay\n",
        "    logging_dir=\"./logs\",            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    save_steps=500\n",
        ")\n",
        "\n",
        "# Define a custom compute_loss function for the Trainer\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): # Add num_items_in_batch argument\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 4: Inference and Task-Specific Functions\n",
        "# Generate financial statements from raw transactional data\n",
        "def generate_financial_statement(data):\n",
        "    input_text = tokenizer(\"Generate financial statement from data:\\n\" + str(data), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    # Increase max_length or set max_new_tokens\n",
        "    output = model.generate(\n",
        "        input_ids=input_text.input_ids,\n",
        "        attention_mask=input_text.attention_mask,\n",
        "        max_length=512,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def summarize_financial_report(report_text):\n",
        "    input_text = tokenizer(\"Summarize key financial insights:\\n\" + report_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    output = model.generate(\n",
        "        input_ids=input_text.input_ids,\n",
        "        attention_mask=input_text.attention_mask,\n",
        "        max_length=150,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def answer_query(query):\n",
        "    input_text = tokenizer(\"Query: \" + query, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    output = model.generate(\n",
        "        input_ids=input_text.input_ids,\n",
        "        attention_mask=input_text.attention_mask,\n",
        "        max_length=100,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "result = generate_financial_statement(structured_transactions.head().to_dict())\n",
        "summary = summarize_financial_report(\"Company X's revenue grew by 20% in Q3, driven by...\")\n",
        "query_result = answer_query(\"What were the effects of the 2008 recession on global markets?\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./financial_gpt2\")\n",
        "tokenizer.save_pretrained(\"./financial_gpt2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ib5sJaC7yRIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results:\n",
        "print(\"Financial Statement:\\n\", result)\n",
        "print(\"\\nSummary:\\n\", summary)\n",
        "print(\"\\nQuery Result:\\n\", query_result)\n",
        "\n",
        "# Further processing (example):\n",
        "# You can store the results in a file, analyze the text,\n",
        "# or use them as input to other functions or models.\n",
        "with open(\"financial_statement.txt\", \"w\") as f:\n",
        "    f.write(result)"
      ],
      "metadata": {
        "id": "tEEyHAJ3wOqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}